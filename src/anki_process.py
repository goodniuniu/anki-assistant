import pandas as pd
import google.generativeai as genai
import os
import time
import json
import logging
import argparse
from pathlib import Path
from tqdm import tqdm
import re
from typing import Dict, Optional

# ================= AI æœåŠ¡å•†æ¥å£ =================
class AIProvider:
    """AI æœåŠ¡å•†åŸºç±»"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)

    def generate_content(self, prompt: str) -> str:
        """ç”Ÿæˆå†…å®¹ï¼Œå­ç±»å¿…é¡»å®ç°"""
        raise NotImplementedError

class GeminiProvider(AIProvider):
    """Google Gemini æœåŠ¡å•†"""

    def __init__(self, config: Dict):
        super().__init__(config)
        os.environ["GOOGLE_API_KEY"] = config["api_key"]
        genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
        self.model = genai.GenerativeModel(config['model'])
        self.logger.info(f"âœ… å·²åˆå§‹åŒ– Gemini æ¨¡å‹: {config['model']}")

    def generate_content(self, prompt: str) -> str:
        """ä½¿ç”¨ Gemini ç”Ÿæˆå†…å®¹"""
        response = self.model.generate_content(prompt)
        return response.text

class QiniuProvider(AIProvider):
    """ä¸ƒç‰›äº‘ AI æœåŠ¡å•†ï¼ˆDeepSeekï¼‰"""

    def __init__(self, config: Dict):
        super().__init__(config)
        try:
            from openai import OpenAI
            self.client = OpenAI(
                base_url=config["base_url"],
                api_key=config["api_key"]
            )
            self.model = config['model']
            self.logger.info(f"âœ… å·²åˆå§‹åŒ–ä¸ƒç‰›äº‘ AI æ¨¡å‹: {config['model']}")
        except ImportError:
            raise ImportError("è¯·å®‰è£… openai åº“: pip install openai")

    def generate_content(self, prompt: str) -> str:
        """ä½¿ç”¨ä¸ƒç‰›äº‘ AI ç”Ÿæˆå†…å®¹"""
        messages = [{"role": "user", "content": prompt}]
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            stream=False,
            max_tokens=4096
        )
        return response.choices[0].message.content

def create_ai_provider(config: Dict) -> AIProvider:
    """
    å·¥å‚æ–¹æ³•ï¼šæ ¹æ®é…ç½®åˆ›å»ºå¯¹åº”çš„ AI æœåŠ¡å•†å®ä¾‹
    """
    provider_name = config.get("provider", "gemini").lower()

    if provider_name == "gemini":
        if "gemini" not in config:
            raise ValueError("é…ç½®ä¸­ç¼ºå°‘ gemini é…ç½®é¡¹")
        return GeminiProvider(config["gemini"])

    elif provider_name in ["qiniu", "deepseek"]:
        if "qiniu" not in config:
            raise ValueError("é…ç½®ä¸­ç¼ºå°‘ qiniu é…ç½®é¡¹")
        return QiniuProvider(config["qiniu"])

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æœåŠ¡å•†: {provider_name}ï¼Œè¯·é€‰æ‹© 'gemini' æˆ– 'qiniu'")

# ================= é…ç½®åŠ è½½ =================
def load_config(config_file='config.json'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼")
        print(f"ğŸ’¡ è¯·å¤åˆ¶ config.example.json ä¸º config.json å¹¶å¡«å†™ä½ çš„ API KEY")
        raise
    except json.JSONDecodeError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        raise

# ================= åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ =================
def setup_logging(log_file):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

# ================= æ•°æ®å‡†å¤‡ =================
def load_input_data(source):
    """
    ä»å¤šç§æ¥æºåŠ è½½è¾“å…¥æ•°æ®
    æ”¯æŒ: list, .txt, .csv, .xlsx
    """
    logger = logging.getLogger(__name__)

    if isinstance(source, list):
        logger.info(f"ä»åˆ—è¡¨åŠ è½½ {len(source)} æ¡æ•°æ®")
        return pd.DataFrame(source, columns=['Front'])

    source_path = Path(source)
    if not source_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {source}")

    if source_path.suffix == '.txt':
        with open(source, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
        logger.info(f"ä» TXT æ–‡ä»¶åŠ è½½ {len(lines)} æ¡æ•°æ®")
        return pd.DataFrame(lines, columns=['Front'])

    elif source_path.suffix == '.csv':
        df = pd.read_csv(source)
        if 'Front' not in df.columns:
            df = pd.DataFrame(df.iloc[:, 0], columns=['Front'])
        logger.info(f"ä» CSV æ–‡ä»¶åŠ è½½ {len(df)} æ¡æ•°æ®")
        return df

    elif source_path.suffix in ['.xlsx', '.xls']:
        df = pd.read_excel(source)
        if 'Front' not in df.columns:
            df = pd.DataFrame(df.iloc[:, 0], columns=['Front'])
        logger.info(f"ä» Excel æ–‡ä»¶åŠ è½½ {len(df)} æ¡æ•°æ®")
        return df

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {source_path.suffix}")

def prepare_data(raw_data):
    """
    å°†åŸå§‹æ•°æ®è½¬æ¢ä¸º DataFrameï¼Œé¢„ç•™ä¸‰åˆ—ç»“æ„
    """
    df = load_input_data(raw_data)
    if 'Back' not in df.columns:
        df['Back'] = ""
    if 'Note' not in df.columns:
        df['Note'] = ""
    return df

# ================= å¤§æ¨¡å‹ä¿¡æ¯è¡¥å……ï¼ˆå¸¦é‡è¯•å’Œæ–­ç‚¹ç»­ä¼ ï¼‰ =================
def clean_json_response(response_text):
    """æ¸…ç†å¤§æ¨¡å‹è¿”å›çš„ JSON å­—ç¬¦ä¸²"""
    # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
    clean_text = response_text.replace('```json', '').replace('```', '').strip()
    # ç§»é™¤å¯èƒ½çš„æ³¨é‡Š
    clean_text = re.sub(r'//.*?\n', '\n', clean_text)
    clean_text = re.sub(r'/\*.*?\*/', '', clean_text, flags=re.DOTALL)
    return clean_text

def call_ai_with_retry(ai_provider: AIProvider, prompt: str, max_retries: int = 3, delay: float = 2):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„ AI è°ƒç”¨
    """
    logger = logging.getLogger(__name__)

    for attempt in range(max_retries):
        try:
            response_text = ai_provider.generate_content(prompt)
            return response_text
        except Exception as e:
            logger.warning(f"AI è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
            if attempt < max_retries - 1:
                time.sleep(delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
            else:
                raise

def enrich_data_with_llm(df, config, logger):
    """
    éå† DataFrameï¼Œè®©å¤§æ¨¡å‹ä¸ºæ¯ä¸€è¡Œè¡¥å……ä¿¡æ¯
    æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    """
    # åˆ›å»º AI æœåŠ¡å•†å®ä¾‹
    try:
        ai_provider = create_ai_provider(config)
        provider_name = config.get("provider", "gemini")
        logger.info(f"ä½¿ç”¨ AI æœåŠ¡å•†: {provider_name}")
    except Exception as e:
        logger.error(f"åˆå§‹åŒ– AI æœåŠ¡å•†å¤±è´¥: {e}")
        raise

    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜
    cache_file = config.get('cache_filename', 'progress_cache.csv')
    start_index = 0

    if Path(cache_file).exists():
        logger.info(f"å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œä»æ–­ç‚¹ç»§ç»­...")
        cache_df = pd.read_csv(cache_file)
        start_index = len(cache_df)
        df = pd.concat([cache_df, df.iloc[start_index:]], ignore_index=True)
        logger.info(f"å·²å®Œæˆ {start_index} æ¡ï¼Œå‰©ä½™ {len(df) - start_index} æ¡")

    logger.info(f"å¼€å§‹å¤„ç† {len(df)} æ¡æ•°æ®ï¼ˆä»ç¬¬ {start_index + 1} æ¡å¼€å§‹ï¼‰...")

    prompt_template = """
ä½ æ˜¯ä¸€ä¸ªè¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹å¥å­ï¼š
"{sentence}"

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡å¹¶ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
1. translate: æä¾›åœ°é“çš„ä¸­æ–‡ç¿»è¯‘ã€‚
2. meta_info: æ¨æµ‹æˆ–æŸ¥æ‰¾è¯¥å¥å­çš„ä½œè€…ã€å‡ºå¤„ï¼ˆä¹¦å/ç”µå½±åï¼‰ä»¥åŠç®€çŸ­çš„èƒŒæ™¯ã€‚å¦‚æœå®Œå…¨æ— æ³•è€ƒè¯ï¼Œè¯·æ ¹æ®å¥å­å†…å®¹é€šè¿‡"AIè§£æ"æ¥è§£é‡Šå…¶è¯­å¢ƒã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
{{
    "translate": "è¿™æ˜¯ä¸­æ–‡ç¿»è¯‘ã€‚",
    "meta_info": "ä½œè€…: XXX <br> å‡ºå¤„: ã€ŠXXXã€‹ <br> èƒŒæ™¯: è¿™å¥è¯é€šå¸¸ç”¨äº..."
}}
æ³¨æ„ï¼šmeta_info ä¸­çš„æ¢è¡Œè¯·ä½¿ç”¨ <br> æ ‡ç­¾ï¼Œå› ä¸ºè¿™æ˜¯ä¸ºäº†å¯¼å…¥ Ankiã€‚
"""

    save_interval = config.get('save_interval', 10)
    request_delay = config.get('request_delay', 1.0)
    max_retries = config.get('max_retries', 3)

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    for index in tqdm(range(start_index, len(df)), desc="å¤„ç†è¿›åº¦"):
        if pd.notna(df.loc[index, 'Back']) and df.loc[index, 'Back'] != "":
            logger.info(f"è·³è¿‡å·²å¤„ç†çš„ç¬¬ {index + 1} æ¡")
            continue

        sentence = df.loc[index, 'Front']

        try:
            # 1. ç”Ÿæˆå†…å®¹
            response_text = call_ai_with_retry(
                ai_provider,
                prompt_template.format(sentence=sentence),
                max_retries=max_retries
            )

            # 2. æ¸…æ´—æ•°æ®
            clean_json = clean_json_response(response_text)

            # 3. è§£æ JSON
            data = json.loads(clean_json)

            # 4. å­˜å…¥ DataFrame
            df.loc[index, 'Back'] = data.get("translate", "ç¿»è¯‘å¤±è´¥")
            df.loc[index, 'Note'] = data.get("meta_info", "æ— é¢å¤–ä¿¡æ¯")

            logger.info(f"âœ… ç¬¬ {index + 1}/{len(df)} æ¡å¤„ç†æˆåŠŸ")

            # 5. å®šæœŸä¿å­˜è¿›åº¦
            if (index + 1) % save_interval == 0:
                df.to_csv(cache_file, index=False)
                logger.info(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼ˆå·²å®Œæˆ {index + 1} æ¡ï¼‰")

            # é¿å…è§¦å‘ API é€Ÿç‡é™åˆ¶
            time.sleep(request_delay)

        except json.JSONDecodeError as e:
            logger.error(f"âŒ ç¬¬ {index + 1} æ¡ JSON è§£æå¤±è´¥: {e}")
            df.loc[index, 'Back'] = "éœ€äººå·¥æ£€æŸ¥"
            df.loc[index, 'Note'] = f"JSON è§£æé”™è¯¯: {str(e)[:100]}"
        except Exception as e:
            logger.error(f"âŒ ç¬¬ {index + 1} æ¡å¤„ç†å¤±è´¥: {e}")
            df.loc[index, 'Back'] = "éœ€äººå·¥æ£€æŸ¥"
            df.loc[index, 'Note'] = f"API Error: {str(e)[:100]}"

    # æœ€ç»ˆä¿å­˜
    df.to_csv(cache_file, index=False)
    logger.info("ğŸ’¾ æœ€ç»ˆè¿›åº¦å·²ä¿å­˜")

    return df

# ================= å¯¼å‡ºä¸º Anki æ ¼å¼ =================
def export_to_anki(df, filename):
    """
    å°†æ•°æ®å¯¼å‡ºä¸º Anki å¯è¯†åˆ«çš„ TXT æ–‡ä»¶
    """
    logger = logging.getLogger(__name__)

    # åˆ›å»ºå‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸæ•°æ®
    export_df = df.copy()

    # æ›¿æ¢æ¢è¡Œç¬¦ï¼Œé˜²æ­¢ç ´å CSV ç»“æ„
    for col in ['Front', 'Back', 'Note']:
        export_df[col] = export_df[col].astype(str).str.replace('\n', '<br>', regex=False)
        export_df[col] = export_df[col].astype(str).str.replace('\r', '', regex=False)
        export_df[col] = export_df[col].astype(str).str.replace('\t', '    ', regex=False)

    # å¯¼å‡º
    export_df.to_csv(filename, sep='\t', index=False, header=False, encoding='utf-8')
    logger.info(f"âœ… æ–‡ä»¶å·²ä¿å­˜: {filename}")
    logger.info(f"ğŸ“Š å…± {len(df)} å¼ å¡ç‰‡")

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("\n" + "="*50)
    print("å¤„ç†å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"  æ€»å¡ç‰‡æ•°: {len(df)}")
    print(f"  æˆåŠŸå¤„ç†: {len(df[df['Back'] != 'éœ€äººå·¥æ£€æŸ¥'])}")
    print(f"  éœ€äººå·¥æ£€æŸ¥: {len(df[df['Back'] == 'éœ€äººå·¥æ£€æŸ¥'])}")
    print(f"  å¯¼å‡ºæ–‡ä»¶: {filename}")
    print("="*50)

# ================= ä¸»ç¨‹åº =================
def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='Anki å¡ç‰‡è‡ªåŠ¨ç”Ÿæˆå·¥å…· - ä½¿ç”¨ AI ç”Ÿæˆç¿»è¯‘å’ŒèƒŒæ™¯ä¿¡æ¯',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä»æ–‡ä»¶ç”Ÿæˆå¡ç‰‡
  python anki_process.py -i input.txt

  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶å
  python anki_process.py -i input.txt -o my_cards.txt

  # ä½¿ç”¨ä¸åŒçš„é…ç½®æ–‡ä»¶
  python anki_process.py -i input.txt -c custom_config.json

  # ä½¿ç”¨å†…åµŒç¤ºä¾‹æ•°æ®
  python anki_process.py --demo

  # æ¸…é™¤ç¼“å­˜é‡æ–°ç”Ÿæˆ
  python anki_process.py -i input.txt --clear-cache
        """
    )

    parser.add_argument(
        '-i', '--input',
        type=str,
        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (æ”¯æŒ .txt, .csv, .xlsx)'
    )

    parser.add_argument(
        '-o', '--output',
        type=str,
        help='è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: anki_cards.txt)'
    )

    parser.add_argument(
        '-c', '--config',
        type=str,
        default='config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)'
    )

    parser.add_argument(
        '--provider',
        type=str,
        choices=['gemini', 'qiniu'],
        help='å¼ºåˆ¶æŒ‡å®š AI æœåŠ¡å•† (è¦†ç›–é…ç½®æ–‡ä»¶)'
    )

    parser.add_argument(
        '--demo',
        action='store_true',
        help='ä½¿ç”¨å†…ç½®çš„ç¤ºä¾‹æ•°æ®è¿è¡Œ'
    )

    parser.add_argument(
        '--clear-cache',
        action='store_true',
        help='æ¸…é™¤ç¼“å­˜æ–‡ä»¶ï¼Œé‡æ–°ç”Ÿæˆæ‰€æœ‰å†…å®¹'
    )

    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='ç¦ç”¨ç¼“å­˜åŠŸèƒ½'
    )

    return parser.parse_args()

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    try:
        # 1. åŠ è½½é…ç½®
        config = load_config(args.config)

        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
        if args.provider:
            config['provider'] = args.provider
        if args.output:
            config['output_filename'] = args.output
        if args.no_cache:
            config['save_interval'] = float('inf')  # è®¾ç½®ä¸ºæ— é™å¤§ï¼Œç¦ç”¨ç¼“å­˜ä¿å­˜

        # 2. è®¾ç½®æ—¥å¿—
        logger = setup_logging(config.get('log_file', 'anki_process.log'))
        logger.info("="*50)
        logger.info("Anki å¡ç‰‡ç”Ÿæˆç¨‹åºå¯åŠ¨")
        logger.info(f"AI æœåŠ¡å•†: {config.get('provider', 'gemini')}")
        logger.info("="*50)

        # 3. æ¸…é™¤ç¼“å­˜ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        cache_file = config.get('cache_filename', 'progress_cache.csv')
        if args.clear_cache and Path(cache_file).exists():
            logger.info(f"æ¸…é™¤ç¼“å­˜æ–‡ä»¶: {cache_file}")
            os.remove(cache_file)

        # 4. å‡†å¤‡æ•°æ®
        if args.demo:
            # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
            logger.info("ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
            raw_data = [
                "To be, or not to be, that is the question.",
                "Stay hungry, stay foolish.",
                "It was the best of times, it was the worst of times.",
                "I'm gonna make him an offer he can't refuse."
            ]
        elif args.input:
            # ä»æ–‡ä»¶è¯»å–
            input_file = args.input
            if not Path(input_file).exists():
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            logger.info(f"ä»æ–‡ä»¶è¯»å–æ•°æ®: {input_file}")
            raw_data = input_file
        else:
            # æ²¡æœ‰æŒ‡å®šè¾“å…¥ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            print("é”™è¯¯: å¿…é¡»æŒ‡å®šè¾“å…¥æ–‡ä»¶æˆ–ä½¿ç”¨ --demo é€‰é¡¹")
            print("\nä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
            print("\nå¿«é€Ÿå¼€å§‹:")
            print("  python anki_process.py --demo          # ä½¿ç”¨ç¤ºä¾‹æ•°æ®")
            print("  python anki_process.py -i input.txt    # ä»æ–‡ä»¶ç”Ÿæˆ")
            return 1

        logger.info("å¼€å§‹å‡†å¤‡æ•°æ®...")
        df = prepare_data(raw_data)
        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…± {len(df)} æ¡")

        # 5. AI è¡¥å……
        logger.info("å¼€å§‹ AI ä¿¡æ¯è¡¥å……...")
        df_enriched = enrich_data_with_llm(df, config, logger)

        # 6. æ‰“å°é¢„è§ˆ
        print("\n--- æ•°æ®é¢„è§ˆï¼ˆå‰3æ¡ï¼‰---")
        print(df_enriched.head(3).to_string())

        # 7. å¯¼å‡º
        output_file = config.get('output_filename', 'anki_cards.txt')
        export_to_anki(df_enriched, output_file)

        logger.info("="*50)
        logger.info("ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        logger.info("="*50)

        return 0

    except FileNotFoundError as e:
        logging.getLogger(__name__).error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print(f"\n[é”™è¯¯] æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return 1
    except Exception as e:
        logging.getLogger(__name__).error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"\n[é”™è¯¯] ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
